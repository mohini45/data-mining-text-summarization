{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextSummerization.ipynb","provenance":[{"file_id":"1lmrgbR0xS5eBHMKWtVVqz_Ofopxru5Ot","timestamp":1649871381874}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Steps\n","- Cleaning text\n","- Word Tokenization\n","- Word frequency count\n","- Normalize the word frequency count\n","- Sentence Tokenization\n","- Sentence score calculation : by adding up word frequency count for each word\n","- get 30% of the sentences  "],"metadata":{"id":"kZNTwjuPwSi0"}},{"cell_type":"markdown","source":["Connecting to Google Drive"],"metadata":{"id":"DTz22bwd3ccx"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"s4LyxvPbZI4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlaPxtzwh4_M","executionInfo":{"status":"ok","timestamp":1650325553061,"user_tz":420,"elapsed":19721,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"d4fce1b1-4b7b-4c51-8290-2dec6e5d69c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0z4JuKQplrh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651082687680,"user_tz":300,"elapsed":48956,"user":{"displayName":"Harshini Gadige","userId":"01067820649925310221"}},"outputId":"2830438c-61ed-4590-a338-0a32f0f125a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Collecting spacy\n","  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 5.6 MB/s \n","\u001b[?25hCollecting typing-extensions<4.0.0.0,>=3.7.4\n","  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Collecting srsly<3.0.0,>=2.4.1\n","  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n","\u001b[K     |████████████████████████████████| 457 kB 32.0 MB/s \n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Collecting spacy-legacy<3.1.0,>=3.0.8\n","  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n","Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 15.0 MB/s \n","\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n","\u001b[K     |████████████████████████████████| 653 kB 34.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n","Collecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 25.5 MB/s \n","\u001b[?25hRequirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.1.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n","Collecting typer<0.5.0,>=0.3.0\n","  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n","Collecting pathy>=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s \n","\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n","Installing collected packages: typing-extensions, catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.2.0\n","    Uninstalling typing-extensions-4.2.0:\n","      Successfully uninstalled typing-extensions-4.2.0\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 typing-extensions-3.10.0.2\n","Collecting en-core-web-sm==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n","\u001b[K     |████████████████████████████████| 13.9 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 2.2.5\n","    Uninstalling en-core-web-sm-2.2.5:\n","      Successfully uninstalled en-core-web-sm-2.2.5\n","Successfully installed en-core-web-sm-3.2.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["# !pip install -U spacy\n","# !python -m spacy download en_core_web_sm"]},{"cell_type":"code","source":["# !pip install google-search-results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OoEOZp0Benz","executionInfo":{"status":"ok","timestamp":1650325676732,"user_tz":420,"elapsed":4988,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"2d348c34-7666-4936-d086-cb4b5ee56c9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting google-search-results\n","  Downloading google_search_results-2.4.1.tar.gz (11 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from google-search-results) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->google-search-results) (3.0.4)\n","Building wheels for collected packages: google-search-results\n","  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-search-results: filename=google_search_results-2.4.1-py3-none-any.whl size=25789 sha256=9e5bdc472a5eecc067c4c8e254d21c150ee51cbd2116177d08109001bf343616\n","  Stored in directory: /root/.cache/pip/wheels/82/a3/c5/364155118f298722dff2f79ae4dd7c91e92b433ad36d6f7e0e\n","Successfully built google-search-results\n","Installing collected packages: google-search-results\n","Successfully installed google-search-results-2.4.1\n"]}]},{"cell_type":"code","source":["# !pip install pdfminer.six"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IpIDGr5qFNcy","executionInfo":{"status":"ok","timestamp":1650325782958,"user_tz":420,"elapsed":7009,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"da2e1f1b-b811-4d3f-a348-02558c315269"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdfminer.six\n","  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n","Collecting cryptography\n","  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 31.3 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n","Installing collected packages: cryptography, pdfminer.six\n","Successfully installed cryptography-36.0.2 pdfminer.six-20220319\n"]}]},{"cell_type":"code","source":["# !pip install rouge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q1OmW3_ZF1x3","executionInfo":{"status":"ok","timestamp":1650325944866,"user_tz":420,"elapsed":5850,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"1a019e5d-dfb0-4ab5-a623-5eaaf55aaed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n"]}]},{"cell_type":"code","source":["import spacy\n","from heapq import nlargest\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from string import punctuation\n","from serpapi import GoogleSearch\n","import os, urllib.request\n","import nltk\n","import re\n","import io\n","from io import StringIO\n","from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n","from pdfminer.converter import TextConverter\n","from pdfminer.layout import LAParams\n","from pdfminer.pdfpage import PDFPage\n","import sys, getopt\n","# from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem.snowball import SnowballStemmer\n","from PIL import Image\n","# nltk.download(\"stopwords\")\n","# nltk.download(\"punkt\")\n","\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from rouge import Rouge"],"metadata":{"id":"bSuReTxenvoC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed6cfaa4-f03a-40c5-a772-3b350abbb79f","executionInfo":{"status":"ok","timestamp":1650326833510,"user_tz":420,"elapsed":148,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context"],"metadata":{"id":"HG2AwNcCBbYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Downloading pdfs from google search engine"],"metadata":{"id":"V6YMGnvd2CqC"}},{"cell_type":"code","source":["def get_pdfs():\n","  search = GoogleSearch({\n","        \"api_key\": \"3948c6d276b1a902335d80dc905d6cb45b14d7056bb017ddbe00a5b01b9d648b\",\n","        \"engine\": \"google\",\n","        \"q\": \"wildfire detection arxiv:pdf\",\n","        \"hl\": \"en\"\n","      })\n","\n","  results = search.get_dict()\n","  count = 0\n","\n","  for index, result in enumerate(results['organic_results']):\n","        if 'pdf' in result['link']:\n","          pdf_file = result['link']\n","\n","          opener=urllib.request.build_opener()\n","          opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582')]\n","          urllib.request.install_opener(opener)\n","\n","          # save PDF\n","          if count < 3:\n","            urllib.request.urlretrieve(pdf_file, f\"/content/drive/MyDrive/data240/Data 240 Project/final_text_summarization/data/pdf_file_{index}.pdf\")\n","            count += 1\n","            print(f'Saving PDF №{index}..')\n","        else: pass\n"],"metadata":{"id":"EZ-eTd-GBhem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get_pdfs()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gOAh4ZdE1xf","executionInfo":{"status":"ok","timestamp":1650326155617,"user_tz":420,"elapsed":9369,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"65fbb13d-3df7-402c-dedd-8548a2a81e89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["https://serpapi.com/search\n","Saving PDF №1..\n","Saving PDF №2..\n","Saving PDF №3..\n"]}]},{"cell_type":"markdown","source":["# Converting pdfs into text file"],"metadata":{"id":"uu1TInMx3HZO"}},{"cell_type":"code","source":["\n","#converts pdf, returns its text content as a string\n","def convert(fname, pages=None):\n","    if not pages:\n","        pagenums = set()\n","    else:\n","        pagenums = set(pages)\n","\n","    output = io.StringIO()\n","    manager = PDFResourceManager()\n","    converter = TextConverter(manager, output, laparams=LAParams())\n","    interpreter = PDFPageInterpreter(manager, converter)\n","\n","    infile = open(fname, 'rb')\n","    for page in PDFPage.get_pages(infile, pagenums):\n","        interpreter.process_page(page)\n","    infile.close()\n","    converter.close()\n","    text = output.getvalue()\n","    output.close\n","    return text \n"],"metadata":{"id":"GB6iTLQUg1Su"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Combining all files"],"metadata":{"id":"THH-3OKA3nmL"}},{"cell_type":"code","source":["#converts all pdfs in directory pdfDir, saves all resulting txt files to txtdir\n","def combinePdf(pdfDir, txtDir):\n","    if pdfDir == \"\": pdfDir = os.getcwd() + \"\\\\\" #if no pdfDir passed in \n","    for pdf in os.listdir(pdfDir): #iterate through pdfs in pdf directory\n","        fileExtension = pdf.split(\".\")[-1]\n","        if fileExtension == \"pdf\":\n","            pdfFilename = pdfDir + pdf \n","            text = convert(pdfFilename) #get string of text content of pdf\n","            textFilename = txtDir + pdf + \".txt\"\n","            textFile = open(textFilename, \"w\") #make text file\n","            textFile.write(text) #write text to text file\n","\n","# # set paths accordingly:\n","pdfDir = \"/content/drive/MyDrive/data240/Data 240 Project/final_text_summarization/data/\"\n","txtDir = \"/content/drive/MyDrive/data240/Data 240 Project/final_text_summarization/data/\"\n","combinePdf(pdfDir, txtDir)"],"metadata":{"id":"aQoYkIQ3OnfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir = '/content/drive/MyDrive/data240/Data 240 Project/final_text_summarization/data/'\n","\n","total_text = \"\"\n","for foldername,subfolders,files in os.walk(r\"/content/drive/MyDrive/data240/Data 240 Project/final_text_summarization/data/\"):\n","  for filename in files:\n","    if \".txt\" in filename:    \n","      with open(dir+filename) as infile:\n","          contents = infile.read()\n","          print(f\"Length of the content in file {filename} : {len(contents)}\")\n","          total_text += contents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uwyTHrRj3CR","executionInfo":{"status":"ok","timestamp":1650326630782,"user_tz":420,"elapsed":143,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"bf76685f-7c11-41a2-ea58-3aa536c61987"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of the content in file pdf_file_1.pdf.txt : 27308\n","Length of the content in file pdf_file_2.pdf.txt : 34936\n","Length of the content in file pdf_file_3.pdf.txt : 30158\n"]}]},{"cell_type":"code","source":["print(f\"Length of the total contents combined from all files: {len(total_text)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nOyfURC-k5B4","executionInfo":{"status":"ok","timestamp":1650326696269,"user_tz":420,"elapsed":160,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"aca7cdb5-0054-4a06-a5f6-dabf5b6ebda8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of the total contents combined from all files: 92402\n"]}]},{"cell_type":"markdown","source":["Helper Functions"],"metadata":{"id":"5WSuYQqW58tg"}},{"cell_type":"code","source":["def processText(text):\n","  processedText = re.sub(\"’\", \"'\", text)\n","  processedText = re.sub(\"[^a-zA-Z' ]+\", \" \", processedText) \n","  return processedText"],"metadata":{"id":"nxl1LcHm5-XT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getStopwords():\n","  stopwords_spacy = list(STOP_WORDS)\n","  # print(stopwords_spacy)\n","  return stopwords_spacy"],"metadata":{"id":"lFaxLeEr-aca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Summerization using spacy and heapq libraries"],"metadata":{"id":"PsEPJoMp3u6D"}},{"cell_type":"code","source":["def summarizationLib(total_text):\n","  processedText = processText(total_text)\n","  # get stop words\n","  stopwords_spacy = getStopwords()\n","  # Removing punctuations\n","  str_punctuation = punctuation + '\\n'\n","  # print(punctuation)\n","   # Word Tokenization\n","  spacy_tokenization = spacy.load('en_core_web_sm')\n","  doc = spacy_tokenization(total_text)\n","  # tokens = [token.text for token in doc]\n","  # print(tokens)\n","  # word frequency count\n","  word_frequencies = {}\n","  for word in doc:\n","    if word.text.lower() not in stopwords_spacy:\n","      if word.text.lower() not in str_punctuation:\n","        if word.text not in word_frequencies.keys():\n","          word_frequencies[word.text] = 1\n","        else:\n","          word_frequencies[word.text] += 1\n","  # normalization of word frequencies\n","  max_frequency = max(word_frequencies.values())\n","  for word in word_frequencies.keys():\n","    word_frequencies[word] = word_frequencies[word] / max_frequency\n","\n","  # Sentence tokenization\n","  sentence_tokens = [sent for sent in doc.sents]\n","  # sentence_tokens\n","  # Sentence scores \n","  sentence_scores = {}\n","  for sent in sentence_tokens:\n","    for word in sent:\n","      if word.text.lower() in word_frequencies.keys():\n","        if sent not in sentence_scores.keys():\n","          sentence_scores[sent] = word_frequencies[word.text.lower()]\n","        else:\n","          sentence_scores[sent] += word_frequencies[word.text.lower()]\n","  # get 30% of the sentences with max score \n","  select_len = int(len(sentence_tokens)*0.3)\n","  select_len\n","  # we need to select select_len number of sentences as part of summary\n","  summary = nlargest(select_len,sentence_scores,key=sentence_scores.get)\n","  summary\n","  # combine sentences together\n","  final_summary = [word.text for word in summary]\n","  summary = ' '.join(final_summary)\n","  # print(\"Summary using libraries : \\n\")\n","  # print(summary)\n","  return summary\n","\n"],"metadata":{"id":"QDzvCG9bSj8_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_lib = summarizationLib(total_text)\n","summary_lib"],"metadata":{"id":"PkVlPWGBvvMw","colab":{"base_uri":"https://localhost:8080/","height":142},"outputId":"217c6842-22c6-469f-e373-829476ffc9b0","executionInfo":{"status":"ok","timestamp":1650326896643,"user_tz":420,"elapsed":6126,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'T\\n\\ni\\n\\n104\\n\\n103\\n\\n102\\n\\nCAL FIRE database\\nProposed method\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nNumber of Sensors\\n\\n10\\n105\\n\\n(a)\\n\\n104\\n\\n)\\n\\n2\\nm\\nk\\n(\\na\\ne\\nr\\nA\\n\\n103\\n\\n102\\n\\nCAL FIRE database\\n\\nProposed method\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nNumber of Sensors\\n\\n10\\n105\\n\\n(b)\\n\\n)\\nn\\no\\nt\\n(\\nn\\no\\ni\\ns\\ns\\ni\\nm\\nE\\nn\\no\\nb\\nr\\na\\nC\\n\\n108\\n\\n107\\n\\n106\\n\\nCAL FIRE database\\nProposed method\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nNumber of Sensors\\n\\n10\\n105\\n\\n(c)\\n\\n)\\n\\nD\\nS\\nU\\n\\n(\\n\\ne\\nc\\ni\\nr\\nP\\nn\\no\\nb\\nr\\na\\nC\\n\\n109\\n\\n108\\n\\n107\\n\\nCAL FIRE database\\nProposed method\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nNumber of Sensors\\n\\n10\\n105\\n\\n(d)\\n\\n)\\n\\nD\\nS\\nU\\n\\n(\\ns\\ng\\nn\\ni\\nv\\na\\nS\\n\\n1.12\\n\\n1.1\\n\\n1.08\\n\\n1.06\\n\\n1.04\\n\\n1.02\\n\\n1\\n\\n109\\n\\nPer sensor cost = \\n1\\n0\\n2\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n1\\n\\n \\n \\n \\n \\n \\n \\n\\x0c3\\n3\\n\\n3\\n3\\n\\n4\\n4\\n5\\n5\\n6\\n6\\n7\\n7\\n7\\n7\\n\\n8\\n8\\n8\\n9\\n9\\n12\\n12\\n12\\n13\\n13\\n\\n14\\n14\\n14\\n14\\n15\\n\\n15\\n\\nContents\\n\\n1 Introduction\\n\\n1.1 Research Outline . \\n\\n[19] Climate\\n\\n”ERA5-Land\\n\\nData\\n\\n”Facts\\n\\n+\\nAvailable:\\n\\nhourly\\n\\ndata\\nAvailable:\\n\\nfrom\\nhttps://cds.climate.copernicus.eu/cdsapp#!/dataset/10.24381/cds.e2161bac?tab=overview\\n\\n[Online]. 10\\nPer sensor cost = 20\\nPer sensor cost = 50\\nPer sensor cost = 100\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nNumber of Sensors\\n\\n10\\n105\\n\\n(e)\\n\\nFig. 5. THE ENVIRONMENTAL DATA OF CALIFORNIA IN 2020\\n\\nVariables\\nName\\nDimensions\\nGrid spacing\\nUnit\\n\\nu10\\n10-meter U-wind component\\n111 × 101 × 8784\\n10 (km)\\nm/s\\n\\nv10\\n10-meter V-wind component\\n111 × 101 × 8784\\n10 (km)\\nm/s\\n\\nSwvl1\\nVolumetric soil water layer 1\\n111 × 101 × 8784\\n10 (km)\\n%\\n\\nBiomass\\nAbove-ground live biomass\\n11645 × 10666 × 1\\n100 (m)\\nMg/ha\\n\\n)\\nr\\nu\\no\\nh\\n(\\n\\ne\\nm\\n \\n\\nModel\\n“U-Net”\\n“U-Net-Light”\\nFCN\\n\\nNo. of Parameters\\n34,525,121\\n2,161,649\\n2,138,785\\n\\nPrecision Recall\\n0.888\\n0.861\\n0.985\\n\\n0.898\\n0.908\\n0.878\\n\\nF1\\n0.893\\n0.884\\n0.928\\n\\nF2\\n0.890\\n0.870\\n0.962\\n\\nThe architecture of the FCN required and less regularization than the “U-Net” and “U-Net-Light,” which used dropout\\nlayers between convolutions. g(Ws) is computed by\\n\\ng(Ws) = 1 − (1 − g0) exp\\n\\n−\\n\\n(cid:18)\\n\\nW 2\\ns\\n2500\\n\\n(cid:19)\\n\\n(2)\\n\\nwhere g0 = 0.1 is a constant that controls the growth rate\\nwhen there is no wind. \\n\\n[10] Inmarsat, MediaTek\\n\\nInc.,\\n\\nInstitute\\n\\nfor\\n\\nInformation\\n\\n“Demonstrating NB-IoT\\nand\\nset-up,\\n89E,\\nRP-201702,\\nhttps://www.3gpp.org/ftp/tsg ran/TSG RAN/TSGR 89e/Docs\\n\\nsatellite\\n3GPP\\n[Online]. \\n\\nof\\nEconomy,\\nhttps://cotce.ca.gov/documents/correspondence/staff and commissioners/documents/Carbon%20tax.pdf\\n\\ncarbon\\non\\n\\ncalifornia,”\\n\\nof\\n21st\\n\\ntax\\nthe\\n\\nIbele,\\n\\nthe\\n\\n\\x0c \\n\\nAccordingly, the loss function was deﬁned as:\\n\\nL(θ ) =\\n\\n1\\nm\\n\\n·\\n\\nm\\n∑\\ni=1\\n\\nk\\n∑\\nn=1\\n\\n(cid:104) This\\nclass weight was computed using:\\n\\npc =\\n\\nTotal number of pixels that equal 0\\nTotal number of pixels that equal 1\\n\\n=\\n\\nTotal number of pixels in dataset\\nTotal number of pixels that equal 1\\n\\n− 1. pe = 0.95572\\npb = 0.93266\\n\\nppooled =\\n\\nz =\\n\\n(cid:113)\\n\\npe + pb\\n2\\n\\n=\\n\\npene + pbnb\\nne \\n\\nTABLE I\\nGEO SATELLITE LINK BUDGET PARAMETERS\\n\\nElevation angle (degrees)\\nTransmission mode\\nSubcarrier frequency (GHz)\\nTX: EIRP (dBm)\\nRX: G/T (dB/K)\\nBandwidth (kHz)\\nPLFS (dB)\\nPLA (dB)\\nPLSM (dB)\\nPLS (dB)\\nPLP (dB)\\nDistance (km)\\nCNR (dB)\\n\\n10\\nUL\\n1.5\\n23\\n19\\n3.75\\n188.14\\n0.16\\n3\\n2.2\\n3\\n40581\\n8.3714\\n\\n90\\nUL\\n1.5\\n23\\n19\\n3.75\\n187.05\\n0.16\\n3\\n2.2\\n3\\n35786\\n9.4636\\n\\nburning area, as illustrated in Fig. Preliminary Wildﬁre Detection: A Dataset and\\nChallenges\\n\\nSamarth Shah\\n\\n2021 Summer STEM Institute\\n\\nSeptember 14, 2021\\n\\nAbstract\\n\\nWildﬁres are uncontrolled ﬁres in the environment that can be caused\\nby humans or nature. (1 − βm)2, βm =\\n\\nβroot\\nβe ,\\n1,\\n\\n(\\n\\nfor βroot ≤ βe\\nfor βroot > βe\\n\\n(3)\\n\\nwhere βe = 0.35 is a threshold. \\n\\n12\\n\\n\\x0cAnalyzing Multispectral Satellite Imagery of South American\\nWildﬁres Using Deep Learning\\n\\nChristopher Sun\\n\\nMarch 2022\\n\\nAbstract\\n\\nSince severe droughts are occurring more frequently and lengthening the dry season in the Amazon\\n \\n\\n[2] L. Yu\\n\\nand N. Wang\\n\\nand X. Meng,\\n\\nInternational Conference\\n\\nﬁre detection with wireless\\n2005\\nNetworking and Mobile Computing, 2005. \\n\\nFigure 1: Experimental Design\\n\\n2 Materials and Data\\n\\n2.1 Landsat 8 Imagery\\n\\nThe data used for this research was a subset of a large publicly available database of Landsat 8 satellite images [10]\\nprocessed by Pereira et al. (2021) Available:\\nhttps://www.t-mobile.com/content/dam/tfb/pdf/IoT-Solution-Developer-Protocols-Guide.pdf?icid=TFB TMO P 19IOT E3UOLES40BRO6VPW19260\\n\\nsolution\\n\\nInc.,\\n\\nIoT\\n\\n[17] National\\nNational\\nhttps://www.nationalforests.org/blog/seven-largest-national-forests\\n\\nFoundation. \\n\\n1\\n2\\n0\\n2\\n\\np\\ne\\nS\\n0\\n1\\n\\n]\\n\\nV\\nC\\n. \\n\\n3 Methods\\n\\n3.1 Preprocessing\\n\\nSome 10-band images had a large number of pixels that were 0, the reasons for which were unknown.2 After elimina-\\ntion of these images, the data set was narrowed down to 14,274 “ﬁre” images. \\n\\n2\\n2\\n0\\n2\\n\\nr\\np\\nA\\n3\\n\\n]\\n\\nG\\nL\\n. \\n\\n4 Results\\n\\n4.1 Fully Convolutional Neural Network\\n\\n4.1.1 Learning Curves\\n\\nFigure 6: The train and validation performance of the FCN over 100 epochs. 2.3387\\n\\n= 0.94419\\n\\np = normalCd f (z, ∞, 0, 1) = 0.00968\\n\\n5 Discussion\\n\\n5.1 \\n\\n1981\\n\\nto\\n\\n[21] P. Mark\\n\\n[20] California Department of Forestry and Fire Protection. ” Hence, the F2 score, given below, was selected as\\na performance metric for the FCN.5\\n\\nSubstituting P and R in terms of their components,\\n\\nF2 =\\n\\n5PR\\n4P + R\\n\\n. The ﬁre spreading speed up (m/s)\\ncan be represented as\\n\\nup = umax × g(Ws) × h(βroot)\\n\\n(1)\\n\\nwhere umax is a constant representing the maximum ﬁre\\nspreading speed which is 0.13 m/s (0.45 km/h), and g(Ws)\\nand h(βroot) are functions representing the dependence of up\\non the wind speed (Ws) and root zone soil wetness (βroot). \\n9\\n0\\n1\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\n1\\n\\n \\n \\n \\n \\n \\n \\n\\x0c1\\n\\nIntroduction\\n\\nWildﬁres are uncontrolled ﬁres in the environment that can be caused by\\nhumans or nature. \\n\\n3.2 Fully Convolutional Neural Network\\n\\n3.2.1 Neural Network Design\\n\\nGiven the images and masks in the data set, the ultimate modeling task was to extract features from each image to\\npredict the corresponding mask. \\n\\n3Each mask contained 16,384 pixels\\n\\n5\\n\\n\\x0c3.2.2 Loss and Metric Functions\\n\\nDue to the class imbalance in the image masks, the model required a weighted loss function. Taiwan\\n3MediaTek Inc.\\nEmail: {liuhowhang, rchang}@citi.sinica.edu.tw, yiyingchen@gate.sinica.edu.tw, ik.fu@mediatek.com\\n\\n1\\n2\\n0\\n2\\n\\np\\ne\\nS\\n2\\n2\\n\\n]\\nP\\nS\\n. Fβ =\\n\\n1 + β 2\\nβ 2\\nP + 1\\nR\\n\\n=\\n\\n(1 + β \\n\\n6 Conclusions\\n\\n2\\n\\n\\x0c1 Introduction\\n\\nIn places like the Amazon Rainforest, where severe droughts are occurring more frequently and lengthening the dry\\nseason, interconnected ecosystems are threatened by more widespread occurrences of wildﬁres (Arag˜ao et al. 2018) · log (1 − h(x(n)))\\n\\n(cid:105)i\\n\\n,\\n\\nwhere k is the number of pixels in each ﬂattened mask, y(n) is the ground truth classiﬁcation of the nth pixel, h(x(n)) \\ndoi:10.1109/WCNM.2005.1544272\\n\\nsensor networks,”\\n\\n“Real-time\\n\\nforest\\nin Proceedings. The third term of\\n\\n\\x0c(6) is the free space path loss (PLFS) equal to 188.14 dB when\\nthe elevation angle is 10 degrees, which is calculated from\\n\\nPLFS = 32.45 (dB) + 20 log10(d) + 20 log10(f )\\n\\n(7)\\n\\nwhere f = 1.5 GHz and d = 40581 km from Table I. LB is related to Ws and is represented\\nas\\n\\nLB = 1 + 10\\n\\n1 − exp(−0.017Ws)\\n\\n. \\n\\np\\n\\n10 + v2\\nu2\\n\\nTable II presents the environmental data of California in\\n2020 from [19]. Then, the carbon emission\\nin the unit of ton is given by\\n\\nA × 1.2Bavg × 100\\n\\n(5)\\n\\nFig. \\n\\n4.2 Cirrus Band Analysis\\n\\n4.2.1 Unsupervised Image Segmentation\\n\\nFigure 10: \\n\\nF2 =\\n\\nt p\\nt p + 0.2 f p + 0.8 f n\\n\\n. \\n\\n6\\n\\n\\x0c4 Methodology\\n\\nThe dataset was consists of 2000 images of wildﬁres on landscapes suscep-\\ntible to containing wildﬁres and 2000 images of landscapes that are susceptible\\nto containing wildﬁres but don’t contain any wildﬁres at the instance they were\\ntaken. \\n\\n”Seven\\n\\nLargest\\nAvailable:\\n\\nInc.\\n[18] Insurance\\nStatistics:\\n[Online]. \\n\\nReferences\\n\\n[1] Arag˜ao, L. E., Anderson, L. O., Fonseca, M. G., Rosan, T. M., Vedovato, L. B., Wagner, F. H., ... & Saatchi,\\n\\nS. (2018). \\n\\nThe received carrier-to-noise ratio (CNR) at the satellite for\\n\\nthe uplink transmission can be calculated by [14]\\n\\nCNR (dB) = \\n\\n[15] Physical\\n\\nlayer procedures (FDD), 3GPP TS 25.214 version 15.0.0\\n\\nRelease 15, 2018. \\n\\nthere was a total of 4000 images in the dataset (2000 images of wildﬁres and\\n2000 images of landscapes susceptible to containing wildﬁres, but not containing\\nthem), more images in each of the classes would have helped further increase\\nthe accuracy of the model since the features that were associated with wildﬁres\\nwere not very evident in the images and henceforth adding more data would\\nhave helped improve the robustness of the model. \\n\\nparameters\\nRep.\\n\\nviability\\nkey\\n\\nTech. \\n\\n(cid:0)\\n\\n(cid:1)\\n\\nFig. “Non-ﬁre” images were not included as\\n\\n1Though Landsat 8 Operational Land Imager (OLI) and Thermal Infrared Sensor (TIRS) images have 11 bands, the Panchromatic band with 15\\n\\nmeters resolution was excluded at the discretion of the researchers who processed the data. \\n\\n6 Conclusion\\n\\nThe research for this study emphasizes the need for a more representative and\\nless biased dataset for the purposes of preliminary wildﬁre detection using Con-\\nvolutional Neural Networks that take in data in the form of image data. \\n\\nB. Results and Discussion\\n\\n1) Fire Burning Time/Area vs. the Number of Sensors: We\\nﬁrst examine the effect of the number of sensors on the ﬁre\\nburning time and area. SIMULATION RESULTS\\n\\nA. Simulation Setup: California Wildﬁres\\n\\nWe consider California wildﬁres with fundamental historical\\nwildﬁre data as a case study for our proposed model. \\n\\n[11] R. Beneduce, R. Hill, and C. Schelle, “Alertwildﬁre (group 6),”\\n\\n[12] P. Jain, S. C. P. Coogan, S. G. Subramanian, M. Crowley, S. Taylor, and\\nM. D. Flannigan, “A review of machine learning applications in wildﬁre\\nscience and management,” CoRR, vol. \\n\\n2 Literature Review\\n\\nI have determined some of the practical problems faced by researchers across\\nthe board regarding preliminary wildﬁre detection by looking through several\\npieces of literature relating to real-world applications using Artiﬁcial Intelligence\\nand collecting data that will be used to train the model. \\n\\n3.3.3 Cirrus-to-Wildﬁre CNN\\n\\nTo test the hypothesis that feature simpliﬁcation could provide added beneﬁts to wildﬁre detection algorithms, two\\nconvolutional neural networks were constructed: each with the same architecture but trained on different data features. \\n\\nA. Wildﬁre Model\\n\\nWe extend the elliptical wildﬁre model with a single ignition\\npoint and a constant wind magnitude and direction The images with the largest number of ﬁre pixels (points near the top of the graphs) appear on different regions\\nfrom graph to graph: Speciﬁcally, these images have:\\n\\n• More “Dense Cirrus” pixels than the mean number of “Dense Cirrus” pixels across all images. \\n\\nClass Imbalance The data set contained extreme class imbalance, as the masks of the “ﬁre” images consisted of\\noverwhelmingly more “non-ﬁre” pixels than “ﬁre” pixels. \\n\\nREFERENCES\\n\\n[1] C. Yue, P. Ciais, P. Cadule, K. Thonicke, S. Archibald, B. Poulter,\\nW. M. Hao, S. Hantson, F. Mouillot, P. Friedlingstein, F. Maignan, and\\nN. Viovy, “Modelling the role of ﬁres in the terrestrial carbon balance by\\nincorporating SPITFIRE into the global vegetation model ORCHIDEE\\n- part 1: \\n\\n6 Conclusions\\n\\nThe computer vision models designed in this study are useful for Land Cover Change Detection, which involves\\nmonitoring wildﬁre hot spots periodically and conducting time-series analyses of pixel-wise probabilities to predict\\nwildﬁres in advance. \\n\\n5.1.2 Cirrus-to-Wildﬁre CNN\\n\\nThe success of the Experimental Cirrus-to-Wildﬁre CNN shows that feature simpliﬁcation is a promising way to detect\\nactive wildﬁres. \\n\\n10\\n\\n\\x0cReferences\\n\\n[1] “Facts statistics: Wildﬁres,” Insurance Information Institute. \\n\\nB. Carbon Emission Model\\n\\nGiven the burned area, and the biomass within the area,\\nthe corresponding carbon emission can be derived. \\n\\n14\\n\\n\\x0c5.2 Relationship Between Cirrus Clouds and Wildﬁre Acreage\\n\\nRecall that the pixel values of the cirrus bands were segmented into three cirrus contamination categories: dense,\\nscattered, and none. \\n\\n[13] G. Charbit, A. Medles, P. Jose, D. Lin, X. Zhu, and I.-K. Fu, “Satellite\\nand cellular networks integration – a system overview,” MediaTek Inc.\\n[14] Solutions for NR to support non-terrestrial networks (NTN), 3GPP TS\\n\\n38.821 version 16.1.0 Release 16, 2021. \\n\\n[2] Cragcrest, “Why california’s wildﬁres are so destructive, in 5 charts,” Nov\\n\\n2018. \\n\\n4.1.3 Performance Visualizations\\n\\nThe following ﬁgure shows some examples of the models’ predictions when there are a small number of pixels in the\\nmask that contain an active ﬁre. \\n\\n16\\n\\n\\x0cSensor-Based Satellite IoT for Early Wildﬁre\\nDetection\\n\\nHow-Hang Liu1, Ronald Y. Chang1, Yi-Ying Chen2, and I-Kang Fu3\\n1Research Center for Information Technology Innovation, Academia Sinica, Taiwan\\n2Research Center for Environmental Changes, Academia Sinica, \\n\\n1.1 Research Outline\\n\\nThe following sections of this work detail a comprehensive deep learning study conducted on multispectral geospatial\\ndata of Ecuador and the Galapagos from the Landsat 8 satellite [7]. \\n\\n12\\n\\n\\x0c4.2.2 Learning Curves and Metrics\\n\\nFigure 12: The train and validation performance of the Cirrus-to-Wildﬁre CNN over 250 epochs. \\n\\nIndustry,\\nuselive\\ntrial\\nTSG RAN-\\nAvailable:\\n\\n[11] V. K. Arora and G. J. Boer, “Fire as an interactive component of dynamic\\nvegetation models,” Journal of Geophysical Research, vol. \\n\\n3.3.2 K-Means Clustering for Unsupervised Cirrus Image Segmentation\\n\\nImage segmentation was conducted to simplify the features of the cirrus clouds. \\n\\n3.3.1 Cirrus Data Preparation\\n\\nBefore training a neural network, the cirrus bands needed to be preprocessed. \\n\\n[4] P. b. S. Elliott, D. Voican, and A. Singh, “Drones provide eye-in-the-sky\\n\\nto help ﬁght ﬁres,” Jul 2021. \\n\\n[6] Simplilearn, “What is computer vision: Applications, beneﬁts and how to\\n\\nlearn it,” Simplilearn.com, Apr 2021. \\n\\n4.2.3 Two Proportion Z-Test\\n\\nA one-tailed two proportion z-test was conducted to assess the statistical signiﬁcance of the observed difference in\\nmodel performance between the benchmark and experimental models. \\n\\n[21] S. Saha, “A comprehensive guide to convolutional neural networks–the eli5\\n\\nway,” Dec 2018. \\n\\nC. Satellite Link Budget Analysis\\n\\nHere, we conduct a feasibility check of GEO satellite\\ncommunication supporting sensors on the earth based on the\\n3GPP NB-IoT non-terrestrial network (NTN) solution [13]. \\n\\n[20] T. Contributor, “What is convolutional neural network? - deﬁnition from\\n\\nwhatis.com,” Apr 2018. \\n\\n3.3 Cirrus Band Analysis\\n\\nEnvironmental research has shown that there are subtleties in atmospheric responses to wildﬁres that provide \\n\\nInformation\\nWildﬁres”,\\n\\nInstitute,\\n2021. \\n\\n2 Materials and Data\\n\\n2.1 Landsat 8 Imagery . \\n\\nModel Inputs\\n\\nFigure 11: Inputs for the Experimental CNN. \\n\\n4.1.2 Metrics and Confusion Matrix\\n\\nFigure 7: The confusion matrix of the FCN shows the proportion of real ﬁres identiﬁed correctly (recall)\\nand the proportion of predicted ﬁres identiﬁed correctly (precision). We propose a wildﬁre evolution\\n\\n \\n \\n \\n \\n \\n \\n\\x0c(a)\\n\\n(b)\\n\\nFig. 2. \\n\\n3 Research Questions\\n\\n1. \\n\\n15\\n\\n\\x0cAcknowledgement\\n\\nI would like to thank Mr. Lordan for his time and feedback regarding the research in this paper. \\n\\n4.1.4 Model Testing Scenarios\\n\\nGeographical Test \\n\\nOnly the 14,247 “ﬁre” images were used as data for training the neural network; the 11,848 “non-ﬁre” images were\\nexcluded. \\n\\n5 Results and Discussion\\n\\nA Convolutional Neural Network was trained on the dataset. Although the\\nsensor-based systems could report more timely and precise in-\\nformation, there remain issues to address, such as deployment\\n\\nThis work was supported in part by the Ministry of Science and Technology,\\n\\nTaiwan, under Grant MOST 109-2221-E-001-013-MY3. \\n\\n(4)\\n\\nSince up, ub, and v are speeds, they will be multiplied by a\\ntime parameter to represent the distances of major and minor\\naxes of the ellipse. \\n\\nForest\\nForests”,\\n\\n[Online]. ) − k (dBW/K/Hz)\\n\\n(6)\\n\\nwhere 100 is a unit conversion factor. \\n\\n[2] Arana-Pulido, V., Cabrera-Almeida, F., Perez-Mato, J., Dorta-Naranjo, B. P., Hernandez-Rodriguez, S., &\\n\\nJimenez-Yguacel, E. (2018). \\n\\nC. Further Discussion and Future Directions\\n\\nThe results presented have shown the promise of the pro-\\nposed system. \\n\\nPerformance Visualizations\\n\\n4.1 Fully Convolutional Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n\\nI. INTRODUCTION\\n\\nGlobal warming is a pressing global issue and a result of\\nexcessive green house gas (GHG) emission. Deep Learning Tasks\\n\\n5.1.1 FCN\\n\\nThe results of the FCN show \\n\\n3 Methods\\n\\n. \\n\\n4 Results\\n\\n. \\n\\n5 Discussion\\n\\n5.1 \\n\\n3.2.3 Training\\n\\nThe FCN was optimized using Adam optimization (Kingma and Ba 2014) Of the entire South American continent, aerial images of Ecuador and the\\n\\n3\\n\\n\\x0cGalapagos were selected to train the model, and images of Guyana and Suriname were used as test data for a ﬁnal\\nmodel evaluation. \\nAlthough there was a total of 4000 images in the dataset (2000 images of wild-\\nﬁres and 2000 images of landscapes susceptible to containing wildﬁres, but not\\ncontaining them), more images in each of the classes would have helped further\\nincrease the accuracy of the model since the features that were associated with\\nwildﬁres were not very evident in the images and henceforth adding more data\\nwould have helped improve the robustness of the model. \\n\\nSince the experimental model proved to be more computationally stable while generalizing better on validation data,\\nthere was deﬁnitely a measurable increase in model performance when the segmented cirrus bands were included to\\nforecast the presence of wildﬁres. \\n\\nComparison of FCN with Other Models The 34,525,121-parameter neural network designed by Pereira et al.\\n(2021) was referred to by the authors as the “U-Net,” and their 2,161,649-parameter model was referred to as the\\n“U-Net-Light” \\n\\nThe point of simplifying the features of the image from continuous pixel values to discrete three-class values was\\nto allow a deep learning model to learn the associations between high-level cirrus cloud patterns and the presence of\\nwildﬁres.7 In rural regions of the Amazon Rainforest where computational power is a luxury, it is crucial to minimize\\ncomputational expense while maximizing the accuracy of computer vision models, and image segmentation provided\\na proof of concept for this task. Although\\n\\n8\\n\\n\\x0cFigure 7: Training loss and validation loss drop towards 0 while training ac-\\ncuracy and validation accuracy trend upwards toward 1, demonstrating the\\nmodel’s eﬀectiveness through training and its ability to minimize loss and max-\\nimize accuracy. \\n\\n[7] J. Sharma, O. Granmo, M. Goodwin, and J. T. Fidje, “Deep convolu-\\ntional neural networks for ﬁre detection in images,” in Engineering Appli-\\ncations of Neural Networks - 18th International Conference, EANN 2017,\\nAthens, Greece, August 25-27, 2017, Proceedings (G. Boracchi, L. S. Iliadis,\\nC. Jayne, and A. Likas, eds.), vol. This is an important trait when software is used for real-time wildﬁre\\ndetection in South America, since identifying all regions with ground truth ﬁres (i.e. maximizing recall) is the ﬁrst\\npriority.9\\n\\nA potential use case for the model involves is Land Cover Change Detection. \\n\\n[10] K. Govil, M. L. Welch, J. T. Ball, and C. R. Pennypacker, “Preliminary re-\\nsults from a wildﬁre detection system using deep learning on remote camera\\nimages,” Remote. p\\nt p+ f n , where t p (true positives) is the number of pixels containing wildﬁres that are correctly classi-\\nﬁed by the model, f p (false positives) is the number of pixels without wildﬁres but classiﬁed otherwise by the model,\\nand f n (false negatives) is the number of pixels containing wildﬁres but classiﬁed otherwise by the model. \\n\\n[4] L. Guo and W. Wang and G. Wang and J. Cui, “Research and\\nimplementation of forest ﬁre early warning system based on UWB\\nwireless sensor networks,” in 2010 Second International Conference on\\nCommunication Systems, Networks and Applications, 2010. \\n\\nIn comparison, the annual carbon emission from the 2020\\nCalifornia wildﬁres can be calculated as 10202.04 × (1.2 ×\\n46.6237) × 100 = 5.71 × 107 ton based on (5), where we have\\nadopted the average biomass value of the entire California,\\n46.6237 Mg/ha. \\n\\nwildﬁre detection as any individual in the ﬁeld would want to ensure that they\\ncatch all the wildﬁres that occur without missing any, but at the same time, not\\nwanting to overtrain the model to the point where it would detect false positives\\n[12]. \\n\\nThe data consisted of 14,823 images containing at least one region with an active ﬁre, and 11,848 containing no ﬁres\\nin any pixels. \\nAvailable: doi:10.1109/ICCSNA.2010.5588679\\n\\n[5] Z. Chaczko and F. Ahmad, “Wireless sensor network based system\\nfor ﬁre endangered areas,” in Third International Conference on\\nInformation Technology and Applications (ICITA’05), 2005. There\\n\\n3\\n\\n\\x0cwere 4000 images in which 2000 of them were images of ﬁres and 2000 of them\\nwere images of landscapes. \\n\\n4\\n\\n\\x0cdata for the wildﬁre detection model, for reasons described in Section 3.2.3. These terrestrial-based solutions make way for\\nan Image Recognition algorithm that can constantly scan for ﬁres in multiple\\nlocations that are likely to contain wildﬁres with wide-angle pan-tilt-zoom cam-\\n\\n2\\n\\n\\x0cFigure 2: Left: Landscape Without Fire, Right: Landscape With Fire. \\n\\n[9] P. Barmpoutis, P. Papaioannou, K. Dimitropoulos, and N. Grammalidis,\\n“A review on early forest ﬁre detection systems using optical remote sens-\\ning,” Sensors, vol. \\n\\nModel Robustness refers to the model’s ability to identify the important\\nfeatures that distinguish one class from another in the case of image recognition. \\n\\nHypotheses The null hypothesis was that the validation accuracy of the benchmark model would equal the validation\\naccuracy of the experimental model. The main contributions of this paper include:\\n\\n• We develop a mathematical model and perform the fea-\\nsibility analysis of sensor-based satellite IoT for wildﬁre\\nmonitoring and detection. \\n\\nFigure 13: Scatter plots of the number of ﬁre pixels against the number of pixels in each cirrus\\ncontamination class. \\n\\n1) Elliptical Fire Model: The spread of ﬁre from a single\\nignition point under constant wind condition can be modeled\\nas an ellipse, as depicted in Fig. 2(a). \\n\\n10\\n\\n\\x0cThe following ﬁgure shows some examples of the models’ predictions when there are a large number of pixels in the\\nmask that contain an active ﬁre. Available:\\n\\n[3] L. B. Lentile, Zachary, Holden, A. M. S. Smith, M. J. Falkowski, A. T.\\nHudak, P. Morgan, S. A. Lewis, P. E. Gessler, and N. C. Benson,\\n“Remote sensing techniques to assess active ﬁre characteristics and\\npost-ﬁre effects,” International Journal of Wildland Fire, vol. \\n\\nFrom the ﬁgures above, it appears that the plot of Fire Pixels vs. Dense Cirrus Pixels is similar to a y-axis reﬂection\\nof the plot of Fire Pixels vs. No Cirrus Pixels. \\n\\n[3] N. S. D. of the Earth Lab Analytics Hub, “Bringing tech innovation to\\nwildﬁres: 4 recommendations for smarter ﬁreﬁghting as megaﬁres menace\\nthe us,” Jul 2021. \\n\\nWe consider two typical cases of wildﬁre sensing based\\non the sensor trafﬁc model in [16]: periodic report and ﬁre-\\nevent-triggered report. \\n\\nWe calculate the average biomass by dividing the ﬁre\\nburning area into several (say, N ) smaller homogeneous areas\\nwith biomass bn, n = 1, . . . \\n\\n[8] Y. Luo, L. Zhao, P. Liu, and D. Huang, “Fire smoke detection algorithm\\nbased on motion characteristic and convolutional neural networks,” Multim. \\n\\n[7] F. Zhang, P. Zhao, S. Xu, Y. Wua, X. Yang, and Y. Zhang, “Integrating\\nmultiple factors to optimize watchtower deployment for wildﬁre detec-\\ntion,” Science of the Total Environment, vol. Therefore, this would result in\\n\\n9\\n\\n\\x0ca not very robust model as the data used for training is dissimilar to what is\\nactually seen. If the model learned nothing and\\n\\npredicted a mask of pure 0s for every images, its binary accuracy would be extremely high, but this would be a misinformed metric. \\ns\\ns\\ne\\ne\\n[\\n\\n1\\nv\\n5\\n0\\n5\\n0\\n1\\n. \\n9\\n0\\n1\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract—Frequent and severe wildﬁres have been observed\\nlately on a global scale. Calibration is especially important in the ﬁeld of early\\n\\n4\\n\\n\\x0cFigure 3: Left: Detected Wildﬁre, Has Wildﬁre. The ”experimental” model trained on the segmented inputs and SWIR data\\nachieves a binary accuracy that is 2.306% higher than that of the ”benchmark model” that is trained\\nonly on SWIR data. \\n\\n• Experimental Model (SWIR and Cirrus Data) \\n\\nFigure 3: An example of one 10-band image in the data set. \\n\\nThe benchmark model (introduced in Section 3.3.3) with the 2-channel SWIR inputs achieved a validation binary\\naccuracy of 93.266%. \\n\\nWe approximate the ﬁre burning area covered by the many\\nellipses by the area of a circle, since directly calculating the\\ntotal area of possibly overlapping ellipses is intractable. \\n\\nGeneralization refers to the ability of a trained model to perform predic-\\ntions on unseen/unfamiliar data. \\n\\nNeed For Further Research Further research should investigate whether the proposed relationship between the\\ndegree of cirrus cloud contamination correlates with the presence of an active wildﬁre. \\n\\nIn comparison to other research performed in the ﬁeld of preliminary wildﬁre\\ndetection, the accuracy of this Convolutional Neural Network is a little lower. \\n\\nThough cirrus clouds are sometimes visible in the SWIR bands, adding the segmented cirrus band brought out the\\nfeatures of the clouds so that after min-max normalization was applied, the segmented cirrus channel would have more\\nweight. Active ﬁre detection using\\n\\nLandsat-8/OLI data. \\ns\\nc\\n[\\n\\n2\\nv\\n1\\n7\\n6\\n9\\n0\\n. \\ns\\nc\\n[\\n\\n1\\nv\\n3\\n8\\n0\\n5\\n0\\n. \\n\\nLabel Imbalance refers to the problem faced by a model regarding pre-\\ndictions due to the number of examples for each class in the training set being\\nunbalanced. Hence, in order for the model to\\n\\n4Binary accuracy was not a reasonable metric since image masks contained overwhelmingly more 0s than 1s. The left column depicts the input data, the middle column depicts the ground truth\\n\\n9\\n\\n\\x0cmask, and the right column depicts the mask predicted by the FCN. \\n\\nWe compare our simulation results with the historical wild-\\nﬁre data from the California Department of Forestry and Fire\\nProtection (CAL FIRE) database [20]. \\nThrough reviewing the literature in the same ﬁeld of early wildﬁre detection,\\n\\n5\\n\\n\\x0cFigure 4: A Visualization of The Domain Shift Problem \\n\\nLabel Imbalance is evident in several papers through datasets having relatively\\nmore data in the class with wildﬁres. \\n\\n• Benchmark Model (Only SWIR Data) \\n\\nFigure 1: Left: An Image Captured Using A Satellite-based Solution [3]. \\n\\nThese ﬁndings reveal that there might be an underlying connection between a region’s degree of cirrus contamination\\nand the probability of containing an active wildﬁre in that region, supporting Veselovskii et al. The p-value provides strong evidence that there including the segmented cirrus band\\n\\n8The train set binary accuracy reached 99.8% for both models. \\n\\n[14] Q. Dong, S. Gong, and X. Zhu, “Imbalanced deep learning by minority\\nclass incremental rectiﬁcation,” IEEE Trans. \\n\\n7\\n\\n\\x0cModel Architecture Both of the above models used the same architecture, hyperparameters, and random seed. \\n\\n[8] W. Ejaz, M. A. Azam, S. Saadat, F. Iqbal, and A. Hanan, “Un-\\nmanned aerial vehicles enabled IoT platform for disaster man-\\nagement,” Energies, vol. \\n\\nThe ideas of the authors motivated another avenue of experimentation: extracting features from the cirrus bands\\nfor the binary classiﬁcation of the presence of a wildﬁre, as a proof of concept for feature simpliﬁcation. \\n\\n[9] G. Mei, N. Xu, J. Qin, B. Wang, and P. Qi, “A survey of internet of\\nthings (IoT) for geohazard prevention: Applications, technologies, and\\nchallenges,” IEEE Internet of Things Journal, vol. \\n[16] T-Mobile USA,\\n\\ndeveloper\\n“Narrowband\\nprotocols guide,” T-Mobile, Tech. \\n\\n11\\n\\n\\x0c[16] “pytube.” \\n\\n[12] M. Cairns, S. Brown, E. Helmer, and G. Baumgardner, “Root biomass\\nallocation in the world’s upland forests.” \\n\\nThe FCN achieved a 0.962 F2 score and a 0.928 F1 score on the validation data set, corresponding to a Precision of\\n0.878 and a Recall of 0.989. \\n\\n(a) Elliptical ﬁre model. \\n\\n13\\n\\n\\x0cincreases the accuracy of the Cirrus-to-Wildﬁre CNN. \\n\\n[13] S. Sankaranarayanan, Y. Balaji, A. Jain, S. Lim, and R. Chellappa, “Learn-\\ning from synthetic data: \\n\\n2) Carbon Emission/Price vs. the Number of Sensors: \\n\\n11\\n\\n\\x0cFigure 9: Mask predictions when the number of ground truth ﬁres is large. \\n\\nLimitations Limited computational power, which necessitated the reduction of the data set size, was one constraint\\nfaced throughout the course of this research. \\n\\n[5] A. Faculty, “About alertwildﬁre,” ALERTWildﬁre. \\n\\nVisual observation supported the hypothesis that the K-Means clustering algorithm would ﬁnd centroid values that\\naccount for the differing degrees of cirrus cloud contamination in the cirrus band. \\n\\nmodel that takes into account wind speed and soil wet-\\nness. The benchmark model was trained only on the two Short-wave Infrared\\n\\nbands. \\n\\n[15] “nvseismolab,” YouTube. \\n\\n[17] “Opencv modules.” \\n\\n[19] “Pyautogui’s documentation.” \\n\\n[18] “Beautiful soup documentation.” [11]\\n\\nCalibration refers to the model’s conﬁdence or the probability that its\\npredictions are reliable. The carbon\\n\\nprice is shown in Fig. 5(d), which is 116.4 million USD with\\n105 sensors and 23.4 million USD with 106 sensors. Investigating the Relationship Between Dropout Regularization and\\n\\nModel Complexity in Neural Networks. It was hypothesized that if a K-value of 3 was used, the clustering algorithm would identify\\nspatial regions in the image corresponding to degrees of cloud contamination: “dense” cirrus, “scattered” cirrus, and\\n“no” cirrus. \\n\\n8\\n\\n\\x0cThe FCN quickly converged on the train set while the validation loss still experienced some noise until around epoch\\n50. \\n\\nFigure 8: Mask predictions when the number of ground truth ﬁres is small. Wildﬁre detection using transfer learning on augmented\\n\\ndatasets. EIRP (dBW) + G/T (dB/K)\\n\\n− PLFS (dB) − PLA (dB) − PLSM (dB)\\n− PLS (dB) − PLP (dB)\\n− BW (dBHz The minimum\\nthreshold for the range of pixel values was set to be 500.6 Only 5,420 of the 26,671 original images satisﬁed this\\ncondition, with 2,152 “ﬁre” images and 3,268 “non-ﬁre” images. We propose wildﬁre and\\ncarbon emission models that take into account real environmental\\ndata including wind speed, soil wetness, and biomass, to simulate\\nthe ﬁre spreading process and quantify the ﬁre burning areas,\\ncarbon emissions, and economical beneﬁts of the proposed system\\nagainst the backdrop of recent California wildﬁres. \\n\\nFig. 5(b) shows the burned areas vs. the number of sensors\\nresult. Fluorescence lidar\\n\\nobservations of wildﬁre smoke inside cirrus: \\n\\n6In most images, raw pixel values hovered around 5,000. \\n\\n3) Savings vs. the Number of Sensors: \\n\\n6\\n\\n\\x0cachieve a high F2 score, it must be a very good discriminator between the classiﬁcation of each pixel. \\n\\n7\\n\\n\\x0cFigure 6: The Convolutional Neural Network Structure. \\n\\nfor\\nresults,”\\n2020. \\n\\nThe F-beta score with a beta value of 2 was used as a performance metric for the FCN.4 Let precision How is the dataset collected and why is it comprehensive and inclusive of\\n\\nwildﬁre situations? \\n\\n5.1.1\\n5.1.2 Cirrus-to-Wildﬁre CNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . for the ﬁre\\n\\nwhere all terms are speciﬁed in Table I except the last term\\nwhich is the Boltzmann constant k = −228.6. + nb\\npe − pb\\nppooled(1 − ppooled)( 2\\nn )\\n\\n= A Large-Scale Dataset for Active Fire\\n\\nDetection/Segmentation (Landsat-8). \\n\\nDuring the downsampling phase, 2D convolution layers enlarge the number of channels, while maximum pooling\\nreduces dimensionality along the ﬁrst and second axes of the input. \\n\\nSatellite-based monitoring has been used in detecting wild-\\nﬁres. \\n\\nNon-Fire Images Test The former set of images will be referred to as the “ﬁre” images and the latter as the “non-ﬁre” images. \\n\\nComputer Vision allows for a more eﬃcient and cheap solution to a multitude of\\napplications as it can repetitively carry out tasks at a constant faster pace with\\nless human error [6]. Available:\\n\\n“Proposed\\nCommission\\nSacramento, California, USA. \\n\\nbetween longitudes varies depending on the latitude, California\\nspans about 1012 km at 32◦ N latitude and 902 km at 42◦\\nN latitude horizontally. \\n\\nrobustness, sensor power management, and communication\\nreliability and security [5]. \\n\\nVarious types of sensor-based systems such as observation\\nby watchtowers and unmanned aerial vehicles (UAVs) \\n\\npresent”. The masks were composed of 0s and 1s, where a “0” pixel value\\nindicates the absence of ﬁre and a “1” pixel value indicates the presence of ﬁre in that region. \\n\\nFigure 5: An Illustration of A Label Imbalance and The Involvement of Class\\nRectiﬁcation Loss [14]. Convolutional networks for biomedical image\\n\\nsegmentation. \\n\\nWe propose mathematical models to quantitatively evaluate\\nthe proposed system. \\n\\n5As the F1 score is more widely used, the F1 score will also be provided in Section 4.1.2 to allow for comparison with other published models. The importance of skip connections\\n\\napplications in biomedical image segmentation. \\n4.2.3 Two Proportion Z-Test\\n\\n4.2 Cirrus Band Analysis . The learning rate α as a function of the epoch of training, t, was\\n\\nα(t) = \\n\\n9As mentioned in Section 3.2.2, false negatives are more dangerous than false positives, which can be thought of as false alarms. Further studies in the following directions can\\n\\n \\n \\n \\n \\n \\n \\n \\n\\x0cbe done to enhance the completeness and generality of the\\nproposed system:\\n\\n• \\n\\n[3] Dauphin, Y., & Cubuk, E. D. (2020, September). \\n\\n[10] Pereira, G. H. D. A., Fusioka, A.M., Nassu, B. T., & Minneto, R. (2020). \\n\\nUsing UAVs for disaster management faces two major\\nchallenges in regard to power management and security [8]. \\n\\nH0 : pe − pb = 0 \\n\\n[5] Khryashchev, V., & Larionov, R. (2020, March). \\n\\n2. \\n\\nFigure 2: Landsat WRS. \\n\\nThere is a potential to leverage the advances of wireless\\ncommunication and Internet of Things (IoT) technologies\\nfor geo-hazards prevention [9]. \\nAvailable: doi:10.1109/ICITA.2005.313\\n\\n[6] A. Ko, N. M. Y. Lee, R. P. S. Sham, C. M. \\n\\n[6] Kingma, D. P., & Ba, J. (2014). \\n\\ning reaches any of the sensors. THE PROPOSED SYSTEM\\n\\nFig. \\n\\n3. \\n• Similar numbers of “Scattered Cirrus” pixels as the mean number of “Scattered Cirrus” pixels across all images. \\n\\n[14] Sousa, M. J., Moutinho, A., & Almeida, M. (2020). \\n\\nThe F2 is a weighted harmonic mean of the precision and recall, with more weight given to the recall. dx.doi.org/10.21227/t9gn-y009\\n\\n[11] Pereira, G. H. D. A., Fusioka, A. M., Nassu, B. T., & Minetto, R. (2021). \\n\\n[15] Sun, C., Sharma, J., & Maiti, M. (2021). \\n\\n[8] Matson, M., & Holben, B. (1987). \\n\\n[16] Van Gansbeke, W., Vandenhende, S., Georgoulis, S., & Van Gool, L. (2021). α0 e−0.1t ,\\n\\nwhere α0 was 1e-3 and 0.1 was a hyperparameter. \\n\\n[4] Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., & Pal, C. (2016). Since the distance\\n\\nFig. 4. \\n\\n[17] Veselovskii, I., Hu, Q., Ansmann, A., Goloub, P., Podvin, T., & Korenskiy, M. (2021). \\n\\neras [5]. \\n\\n[7] Knight, E. J., & Knight, G. (2014). \\n\\nFigure 5: \\n\\nFigure 4: The F-beta\\nscore is then:\\n\\n \\n\\n[13] Schroeder, W., Oliva, P., Giglio, L., Quayle, B., Lorenz, E., & Morelli, F. (2016). \\n\\naxis direction of the ellipse. \\n\\n[12] Ronneberger, O., Fischer, P., & Brox, T. (2015, October). \\n\\nRemote sensing, 6(11), 10286-10305. International Journal of Remote\\n\\nSensing, 8(3), 509-516. \\n\\nThe difference between the binary accuracies was 2.306%. In\\n\\nInternational Conference on Learning Representations. \\n\\n2This might have been due to the fact that they were acquired at the borders of WRS path-row grids. \\n\\n[9] Patro, S., & Sahu, K. K. (2015). h(βroot) is described as\\n\\nh(βroot) = Less “No Cirrus” pixels than the mean number of “No Cirrus” pixels across all images. 2)PR\\nβ 2P + R\\n\\n. \\n\\n• \\n\\n• \\n\\n• \\n\\n• \\n\\nClustering Hypothesis\\n \\n\\nJuly 2019. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\nFCN . \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\n. \\n\\nII. \\n\\nIII. \\n\\nIV. CONCLUSION\\n\\n Not having a robust model in the ﬁeld of wildﬁre\\ndetection would decrease the representativeness of the displayed accuracy of the\\nmodel if it refers to its accuracy as its ability to detect wildﬁres in general [10]. Relating the previously brought up skewed image with the topic\\nof generalization, if the model trains on images that have odd orientations, un-\\nrepresentative settings, and other sources of dataset bias, it will not generalize\\nits weights on realistic data that it will input when applied to cameras in an\\noutdoor setting where practically all wildﬁres occur [11]. While other researchers have conducted active wildﬁre detection on data\\nsets with more than one hundred thousand images, this data set only contained 14,274 images. Through carrying out the experimentation phase of this study,\\nbias was mitigated from the dataset through inputting more representative im-\\nages, masking parts of the image that would not declare the diﬀerence between\\na wildﬁre existing or not existing, and inputting an equal number of images with\\nwildﬁres and images without them to ensure Label Imbalance was mitigated. \\nIn turn, this would result in a decrease in validity of the accuracy showcased by\\nseveral of the research papers–in the ﬁeld of preliminary wildﬁre detection–that\\nhave datasets ﬁlled with skewed images that had several other indicators of ﬁres\\nsuch as the orientation of the image, the setting of the image, etc. For instance, if an algorithm\\nwas trained on images that were pointed at the same location/landscape with\\nthe same features, it could have only learned how to detect wildﬁres in that\\nparticular setting and for that reason would not be a very robust model as it\\nwould not be using the features that distinguish an image from having a wildﬁre\\nas opposed to not having one. Each scatter plot below contains\\n5,420 data points, each data point representing one image in the data set for the Cirrus-to-Wildﬁre CNN. The\\ncurrent research in the ﬁeld of preliminary ﬁre detection has several prob-\\nlems related to unrepresentative data being used to train models and their\\nexisting varied amounts of label imbalance in the classes (commonly ﬁre\\nand non-ﬁre) of their dataset. Therefore, this would result in a not very robust model as\\nthe data used for training is dissimilar to what is seen when the model tests on\\nreal-world data. We model the ﬁre development\\nwith changing winds as time evolves based on the elliptical\\nﬁre model. In the realm of wildﬁre detection and mitigation, Label Imbalance\\nmust be taken into account as there is always more data with images of with-\\nout wildﬁres than there are with wildﬁres. By doing this, the dataset is ultimately able to incorporate\\nmore variety in the images of wildﬁres as it contains various angles of them\\nwhile also maintaining frames that contained wildﬁres that are representative\\nof what a real-world input for the model would look like. An Illustration of The Model Robustness Problem – The Model’s\\nTendency To Perform Poorly From “Subtle Diﬀerences” In The Input Image. Since not all images in the data set\\ncontained cirrus cloud contamination (what was desirable for the analysis), images that contained cirrus clouds ﬁrst\\nneeded to be selected. For instance, one\\ncommon issue that circulates through the research in this ﬁeld is the issue of\\nModel Robustness–the model’s ability to identify the important features that\\ndistinguish one class from another in the case of image recognition. Simulation results based on real environmental data\\nof California in 2020 demonstrated that deploying as few as\\none sensor per km2 could reduce the annual carbon emission\\nby more than ten times, and deploying our system could yield\\nsigniﬁcant annual savings of billions of USD due to early ﬁre\\ncontainment. It follows that\\nif the model can already discern exactly which few pixels (out of 16,384) have ﬁres, there is little additional beneﬁt\\nfrom adding more data images that have masks of purely 0’s. \\nSpeciﬁcally, we ﬁrst proposed a wildﬁre model with multiple\\nignition points and varying wind magnitudes and directions,\\nand a method to approximate the ﬁre burning area. The third row shows that the model can fail to identify ﬁre-absent pixels when they\\nare surrounded by ﬁre-containing pixels. Therefore, when applied to\\na real-case scenario on unfamiliar data, a model trained on this dataset can\\nbe more robust and will hence be able to generalize on more applicable data. After the pixels of each cirrus image are segmented into\\ndiscrete classes of cirrus contamination, two more convolutional neural networks are trained to investigate the effect\\nof feature simpliﬁcation on model performance. Moreover,\\nthe two-dimensional image data from satellite-sensor remote\\nsensing cannot present the landscape-level ecological infor-\\nmation such as biomass, soil wetness, and wind speed, which\\nresults in inaccurate detection [3]. Despite this, the model\\nstill showcased an increase in accuracy and minimizing of loss as it progressed\\nthrough the training procedure, which one can note as an initial progression of\\nthe Convolutional Neural Network adapting to and learning the features that\\ndetermine whether or not an image contains a wildﬁre.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["len(summary_lib)"],"metadata":{"id":"-ur49ZN2v0Ow","colab":{"base_uri":"https://localhost:8080/"},"outputId":"027c701c-6501-4958-e993-d7a9d0e173df","executionInfo":{"status":"ok","timestamp":1650326975779,"user_tz":420,"elapsed":162,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["41069"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["### Generate Summary using factor based implementation"],"metadata":{"id":"wi3HJUEWn2CZ"}},{"cell_type":"code","source":["def summarize(text):\n","    # Process text by removing numbers and unrecognized characters\n","    processedText = processText(text)\n","    # get stop words\n","    stopwords_spacy = getStopwords()\n","    # Word tokenization\n","    words = word_tokenize(processedText)\n","    # Normalize words with Porter stemming and build word frequency table\n","    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n","    freqTable = dict()\n","    for word in words:\n","        word = word.lower()\n","        if word in stopwords_spacy:\n","            continue\n","        elif stemmer.stem(word) in freqTable:\n","            freqTable[stemmer.stem(word)] += 1\n","        else:\n","            freqTable[stemmer.stem(word)] = 1\n","    # Normalize every sentence in the text\n","    sentences = sent_tokenize(text)\n","    stemmedSentences = []\n","    sentenceValue = dict()\n","    for sentence in sentences:\n","        stemmedSentence = []\n","        for word in sentence.lower().split():\n","            stemmedSentence.append(stemmer.stem(word))\n","        stemmedSentences.append(stemmedSentence)\n","    # Calculate value of every normalized sentence based on word frequency table\n","    # [:12] helps to save space\n","    for num in range(len(stemmedSentences)):\n","        for wordValue in freqTable:\n","            if wordValue in stemmedSentences[num]:\n","                if sentences[num][:12] in sentenceValue:\n","                    sentenceValue[sentences[num][:12]] += freqTable.get(wordValue)\n","                else:\n","                    sentenceValue[sentences[num][:12]] = freqTable.get(wordValue)\n","    # Determine average value of a sentence in the text\n","    sumValues = 0\n","    for sentence in sentenceValue:\n","        sumValues += sentenceValue.get(sentence)\n","    average = int(sumValues / len(sentenceValue))\n","    # Create summary of text using sentences that exceed the average value by some factor\n","    # This factor can be adjusted to reduce/expand the length of the summary\n","    summary = \"\"\n","    for sentence in sentences:\n","            if sentence[:12] in sentenceValue and sentenceValue[sentence[:12]] > (3.0 * average):\n","                summary += \" \" + \" \".join(sentence.split())\n","    \n","    print(summary)\n","    return summary"],"metadata":{"id":"4lRAJKWOnsuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary = summarize(total_text)\n","summary"],"metadata":{"id":"nRcvB_DSn6HQ","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0ae58c5e-442a-4e84-8bf6-987f5bc9596d","executionInfo":{"status":"ok","timestamp":1650326988052,"user_tz":420,"elapsed":1381,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["92402\n"," Although there are many applications of Machine Learning in the ﬁeld of early wildﬁre detection, many are too costly to implement and/or not feasible to work with. Although there are clouds in the background that could be classiﬁed as smoke, they are not considered as smoke caused by a wildﬁre as they are not coming from the ground. However, these papers have several issues and limitations regarding practicality due to their failure to consider the Machine Learning obstacles. Although the accuracies presented by these papers may seem enticing at ﬁrst glance, they are not valid representations of how well their models would perform when given real-world data from common cameras used for early wild- ﬁre detection, such as the ones provided by ALERTWildﬁre. For instance, one common issue that circulates through the research in this ﬁeld is the issue of Model Robustness–the model’s ability to identify the important features that distinguish one class from another in the case of image recognition. Therefore, this would result in a not very robust model as the data used for training is dissimilar to what is seen when the model tests on real-world data. For instance, keeping the dataset with a more representative setting, lighting, atmosphere, etc. Through carrying out the experimentation phase of this research and closely observing the results, one can note the dataset’s attempt to mitigate the bias found in neighboring papers through its distinguishing features mentioned above. For instance, if an algorithm was trained on images that were pointed at the same location/landscape with the same features, it could have only learned how to detect wildﬁres in that particular setting and for that reason would not be a very robust model as it would not be using the features that distinguish an image from having a wildﬁre as opposed to not having one. Although the left image is a correct prediction as there is a wildﬁre in the image, the right image is an incorrect prediction as there is no wildﬁre in that image. Relating the previously brought up skewed image with the topic of generalization, if the model trains on images that have odd orientations, un- representative settings, and other sources of dataset bias, it will not generalize its weights on realistic data that it will input when applied to cameras in an outdoor setting where practically all wildﬁres occur [11]. Label Imbalance refers to the problem faced by a model regarding pre- dictions due to the number of examples for each class in the training set being unbalanced. Label Imbalance is diﬃcult because models will tend to output majority class predictions and will lean in opposi- tion to the minority class. Label Imbalance is evident in several papers through datasets having relatively more data in the class with wildﬁres. By doing this, the dataset is ultimately able to incorporate more variety in the images of wildﬁres as it contains various angles of them while also maintaining frames that contained wildﬁres that are representative of what a real-world input for the model would look like. By doing this, the dataset is also able to incorporate a larger range of data regarding images of landscapes susceptible to containing wildﬁres. These neighboring research papers, as mentioned in other segments of this paper, have issues regarding Model Robustness and Generalization which often occur from the datasets presented in them that often have a bias due to the unrepresentative setting, atmosphere, lighting, etc. In comparison to other research performed in the ﬁeld of preliminary wildﬁre detection, the accuracy of this Convolutional Neural Network is a little lower. These neighboring research papers, as mentioned in other segments of this paper, have issues regarding model robustness and generalization which often occur from the datasets presented in them that often have a bias due to the unrepresentative setting, atmosphere, lighting, etc. there was a total of 4000 images in the dataset (2000 images of wildﬁres and 2000 images of landscapes susceptible to containing wildﬁres, but not containing them), more images in each of the classes would have helped further increase the accuracy of the model since the features that were associated with wildﬁres were not very evident in the images and henceforth adding more data would have helped improve the robustness of the model. Despite this, the model still showcased an increase in accuracy and minimizing of loss as it progressed through the training procedure, which one can note as an initial progression of the Convolutional Neural Network adapting to and learning the features that determine whether or not an image contains a wildﬁre. 6 Conclusion The research for this study emphasizes the need for a more representative and less biased dataset for the purposes of preliminary wildﬁre detection using Con- volutional Neural Networks that take in data in the form of image data. However, these were often unmentioned in these studies, leading to an inac- curate accuracy and validity showcased. Therefore, this would result in 9 a not very robust model as the data used for training is dissimilar to what is actually seen. Through carrying out the experimentation phase of this study, bias was mitigated from the dataset through inputting more representative im- ages, masking parts of the image that would not declare the diﬀerence between a wildﬁre existing or not existing, and inputting an equal number of images with wildﬁres and images without them to ensure Label Imbalance was mitigated. Although there was a total of 4000 images in the dataset (2000 images of wild- ﬁres and 2000 images of landscapes susceptible to containing wildﬁres, but not containing them), more images in each of the classes would have helped further increase the accuracy of the model since the features that were associated with wildﬁres were not very evident in the images and henceforth adding more data would have helped improve the robustness of the model. Despite this, the model still showcased an increase in accuracy and minimizing of loss as it progressed through the training procedure, which one can note as an initial progression of the Convolutional Neural Network adapting to and learning the features that determine whether or not an image contains a wildﬁre. 3.2 Fully Convolutional Neural Network . 6 Conclusions 2 1 Introduction In places like the Amazon Rainforest, where severe droughts are occurring more frequently and lengthening the dry season, interconnected ecosystems are threatened by more widespread occurrences of wildﬁres (Arag˜ao et al. After the pixels of each cirrus image are segmented into discrete classes of cirrus contamination, two more convolutional neural networks are trained to investigate the effect of feature simpliﬁcation on model performance. 3.2 Fully Convolutional Neural Network 3.2.1 Neural Network Design Given the images and masks in the data set, the ultimate modeling task was to extract features from each image to predict the corresponding mask. However, there were no labels provided for this task, so the criteria for determining whether an image contained cirrus clouds had to be experimentally decided. Visual observation of the cirrus bands made it clear that images with cirrus clouds had a much larger pixel value range than images without cirrus clouds. The point of simplifying the features of the image from continuous pixel values to discrete three-class values was to allow a deep learning model to learn the associations between high-level cirrus cloud patterns and the presence of wildﬁres.7 In rural regions of the Amazon Rainforest where computational power is a luxury, it is crucial to minimize computational expense while maximizing the accuracy of computer vision models, and image segmentation provided a proof of concept for this task. Visual observation supported the hypothesis that the K-Means clustering algorithm would ﬁnd centroid values that account for the differing degrees of cirrus cloud contamination in the cirrus band. H0 : pe − pb = 0 HA : pe − pb > 0 pe = 0.95572 pb = 0.93266 ppooled = z = (cid:113) pe + pb 2 = pene + pbnb ne + nb pe − pb ppooled(1 − ppooled)( 2 n ) = 2.3387 = 0.94419 p = normalCd f (z, ∞, 0, 1) = 0.00968 5 Discussion 5.1 Deep Learning Tasks 5.1.1 FCN The results of the FCN show that despite the class imbalance inherent to the data set, the model is still a highly ef- fective discriminator between ﬁre and non-ﬁre pixels. 6 Conclusions The computer vision models designed in this study are useful for Land Cover Change Detection, which involves monitoring wildﬁre hot spots periodically and conducting time-series analyses of pixel-wise probabilities to predict wildﬁres in advance. However, the long scan period and low resolution of satellites limited the performance of the system [2]. Although the sensor-based systems could report more timely and precise in- formation, there remain issues to address, such as deployment This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST 109-2221-E-001-013-MY3. For simplicity, we approximate the geographic area of California by a 1000 × 1100 km2 area, as illustrated in Fig. For simplicity and to focus our discussion on the contribution of the number of sensors, we assume that the ﬁre is extinguished as soon as the ﬁre spread- TABLE II THE ENVIRONMENTAL DATA OF CALIFORNIA IN 2020 Variables Name Dimensions Grid spacing Unit u10 10-meter U-wind component 111 × 101 × 8784 10 (km) m/s v10 10-meter V-wind component 111 × 101 × 8784 10 (km) m/s Swvl1 Volumetric soil water layer 1 111 × 101 × 8784 10 (km) % Biomass Above-ground live biomass 11645 × 10666 × 1 100 (m) Mg/ha ) r u o h ( e m T i 104 103 102 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (a) 104 ) 2 m k ( a e r A 103 102 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (b) ) n o t ( n o i s s i m E n o b r a C 108 107 106 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (c) ) D S U ( e c i r P n o b r a C 109 108 107 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (d) ) D S U ( s g n i v a S 1.12 1.1 1.08 1.06 1.04 1.02 1 109 Per sensor cost = 10 Per sensor cost = 20 Per sensor cost = 50 Per sensor cost = 100 2 4 6 8 Number of Sensors 10 105 (e) Fig. In comparison, the historical annual burned time, calculated by summing the burning time (the duration between incident created time and incident extinguished time) of all California wildﬁres in 2020 in the CAL FIRE database, is 36716.25 hours. In comparison, the annual carbon emission from the 2020 California wildﬁres can be calculated as 10202.04 × (1.2 × 46.6237) × 100 = 5.71 × 107 ton based on (5), where we have adopted the average biomass value of the entire California, 46.6237 Mg/ha.\n"]},{"output_type":"execute_result","data":{"text/plain":["' Although there are many applications of Machine Learning in the ﬁeld of early wildﬁre detection, many are too costly to implement and/or not feasible to work with. Although there are clouds in the background that could be classiﬁed as smoke, they are not considered as smoke caused by a wildﬁre as they are not coming from the ground. However, these papers have several issues and limitations regarding practicality due to their failure to consider the Machine Learning obstacles. Although the accuracies presented by these papers may seem enticing at ﬁrst glance, they are not valid representations of how well their models would perform when given real-world data from common cameras used for early wild- ﬁre detection, such as the ones provided by ALERTWildﬁre. For instance, one common issue that circulates through the research in this ﬁeld is the issue of Model Robustness–the model’s ability to identify the important features that distinguish one class from another in the case of image recognition. Therefore, this would result in a not very robust model as the data used for training is dissimilar to what is seen when the model tests on real-world data. For instance, keeping the dataset with a more representative setting, lighting, atmosphere, etc. Through carrying out the experimentation phase of this research and closely observing the results, one can note the dataset’s attempt to mitigate the bias found in neighboring papers through its distinguishing features mentioned above. For instance, if an algorithm was trained on images that were pointed at the same location/landscape with the same features, it could have only learned how to detect wildﬁres in that particular setting and for that reason would not be a very robust model as it would not be using the features that distinguish an image from having a wildﬁre as opposed to not having one. Although the left image is a correct prediction as there is a wildﬁre in the image, the right image is an incorrect prediction as there is no wildﬁre in that image. Relating the previously brought up skewed image with the topic of generalization, if the model trains on images that have odd orientations, un- representative settings, and other sources of dataset bias, it will not generalize its weights on realistic data that it will input when applied to cameras in an outdoor setting where practically all wildﬁres occur [11]. Label Imbalance refers to the problem faced by a model regarding pre- dictions due to the number of examples for each class in the training set being unbalanced. Label Imbalance is diﬃcult because models will tend to output majority class predictions and will lean in opposi- tion to the minority class. Label Imbalance is evident in several papers through datasets having relatively more data in the class with wildﬁres. By doing this, the dataset is ultimately able to incorporate more variety in the images of wildﬁres as it contains various angles of them while also maintaining frames that contained wildﬁres that are representative of what a real-world input for the model would look like. By doing this, the dataset is also able to incorporate a larger range of data regarding images of landscapes susceptible to containing wildﬁres. These neighboring research papers, as mentioned in other segments of this paper, have issues regarding Model Robustness and Generalization which often occur from the datasets presented in them that often have a bias due to the unrepresentative setting, atmosphere, lighting, etc. In comparison to other research performed in the ﬁeld of preliminary wildﬁre detection, the accuracy of this Convolutional Neural Network is a little lower. These neighboring research papers, as mentioned in other segments of this paper, have issues regarding model robustness and generalization which often occur from the datasets presented in them that often have a bias due to the unrepresentative setting, atmosphere, lighting, etc. there was a total of 4000 images in the dataset (2000 images of wildﬁres and 2000 images of landscapes susceptible to containing wildﬁres, but not containing them), more images in each of the classes would have helped further increase the accuracy of the model since the features that were associated with wildﬁres were not very evident in the images and henceforth adding more data would have helped improve the robustness of the model. Despite this, the model still showcased an increase in accuracy and minimizing of loss as it progressed through the training procedure, which one can note as an initial progression of the Convolutional Neural Network adapting to and learning the features that determine whether or not an image contains a wildﬁre. 6 Conclusion The research for this study emphasizes the need for a more representative and less biased dataset for the purposes of preliminary wildﬁre detection using Con- volutional Neural Networks that take in data in the form of image data. However, these were often unmentioned in these studies, leading to an inac- curate accuracy and validity showcased. Therefore, this would result in 9 a not very robust model as the data used for training is dissimilar to what is actually seen. Through carrying out the experimentation phase of this study, bias was mitigated from the dataset through inputting more representative im- ages, masking parts of the image that would not declare the diﬀerence between a wildﬁre existing or not existing, and inputting an equal number of images with wildﬁres and images without them to ensure Label Imbalance was mitigated. Although there was a total of 4000 images in the dataset (2000 images of wild- ﬁres and 2000 images of landscapes susceptible to containing wildﬁres, but not containing them), more images in each of the classes would have helped further increase the accuracy of the model since the features that were associated with wildﬁres were not very evident in the images and henceforth adding more data would have helped improve the robustness of the model. Despite this, the model still showcased an increase in accuracy and minimizing of loss as it progressed through the training procedure, which one can note as an initial progression of the Convolutional Neural Network adapting to and learning the features that determine whether or not an image contains a wildﬁre. 3.2 Fully Convolutional Neural Network . 6 Conclusions 2 1 Introduction In places like the Amazon Rainforest, where severe droughts are occurring more frequently and lengthening the dry season, interconnected ecosystems are threatened by more widespread occurrences of wildﬁres (Arag˜ao et al. After the pixels of each cirrus image are segmented into discrete classes of cirrus contamination, two more convolutional neural networks are trained to investigate the effect of feature simpliﬁcation on model performance. 3.2 Fully Convolutional Neural Network 3.2.1 Neural Network Design Given the images and masks in the data set, the ultimate modeling task was to extract features from each image to predict the corresponding mask. However, there were no labels provided for this task, so the criteria for determining whether an image contained cirrus clouds had to be experimentally decided. Visual observation of the cirrus bands made it clear that images with cirrus clouds had a much larger pixel value range than images without cirrus clouds. The point of simplifying the features of the image from continuous pixel values to discrete three-class values was to allow a deep learning model to learn the associations between high-level cirrus cloud patterns and the presence of wildﬁres.7 In rural regions of the Amazon Rainforest where computational power is a luxury, it is crucial to minimize computational expense while maximizing the accuracy of computer vision models, and image segmentation provided a proof of concept for this task. Visual observation supported the hypothesis that the K-Means clustering algorithm would ﬁnd centroid values that account for the differing degrees of cirrus cloud contamination in the cirrus band. H0 : pe − pb = 0 HA : pe − pb > 0 pe = 0.95572 pb = 0.93266 ppooled = z = (cid:113) pe + pb 2 = pene + pbnb ne + nb pe − pb ppooled(1 − ppooled)( 2 n ) = 2.3387 = 0.94419 p = normalCd f (z, ∞, 0, 1) = 0.00968 5 Discussion 5.1 Deep Learning Tasks 5.1.1 FCN The results of the FCN show that despite the class imbalance inherent to the data set, the model is still a highly ef- fective discriminator between ﬁre and non-ﬁre pixels. 6 Conclusions The computer vision models designed in this study are useful for Land Cover Change Detection, which involves monitoring wildﬁre hot spots periodically and conducting time-series analyses of pixel-wise probabilities to predict wildﬁres in advance. However, the long scan period and low resolution of satellites limited the performance of the system [2]. Although the sensor-based systems could report more timely and precise in- formation, there remain issues to address, such as deployment This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST 109-2221-E-001-013-MY3. For simplicity, we approximate the geographic area of California by a 1000 × 1100 km2 area, as illustrated in Fig. For simplicity and to focus our discussion on the contribution of the number of sensors, we assume that the ﬁre is extinguished as soon as the ﬁre spread- TABLE II THE ENVIRONMENTAL DATA OF CALIFORNIA IN 2020 Variables Name Dimensions Grid spacing Unit u10 10-meter U-wind component 111 × 101 × 8784 10 (km) m/s v10 10-meter V-wind component 111 × 101 × 8784 10 (km) m/s Swvl1 Volumetric soil water layer 1 111 × 101 × 8784 10 (km) % Biomass Above-ground live biomass 11645 × 10666 × 1 100 (m) Mg/ha ) r u o h ( e m T i 104 103 102 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (a) 104 ) 2 m k ( a e r A 103 102 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (b) ) n o t ( n o i s s i m E n o b r a C 108 107 106 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (c) ) D S U ( e c i r P n o b r a C 109 108 107 CAL FIRE database Proposed method 2 4 6 8 Number of Sensors 10 105 (d) ) D S U ( s g n i v a S 1.12 1.1 1.08 1.06 1.04 1.02 1 109 Per sensor cost = 10 Per sensor cost = 20 Per sensor cost = 50 Per sensor cost = 100 2 4 6 8 Number of Sensors 10 105 (e) Fig. In comparison, the historical annual burned time, calculated by summing the burning time (the duration between incident created time and incident extinguished time) of all California wildﬁres in 2020 in the CAL FIRE database, is 36716.25 hours. In comparison, the annual carbon emission from the 2020 California wildﬁres can be calculated as 10202.04 × (1.2 × 46.6237) × 100 = 5.71 × 107 ton based on (5), where we have adopted the average biomass value of the entire California, 46.6237 Mg/ha.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["len(summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtwWy1HYJ5PL","executionInfo":{"status":"ok","timestamp":1650327025071,"user_tz":420,"elapsed":142,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}},"outputId":"452ea52e-5a23-4016-b421-d1cafbbc947b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10865"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"kGFrw7SRcX99"}},{"cell_type":"code","source":["human_summary = \"\"\"Wildfires are uncontrolled fires in the environment that can be caused\n","by humans or nature. In 2020 alone, wildfires in California have burned\n","4.2 million acres, damaged 10,500 buildings/structures, and killed more\n","than 31 people, exacerbated by climate change and a rise in average global\n","temperatures [1] [2]. This also means there has been an increase in the\n","costs of extinguishing these treacherous wildfires. The objective of the\n","research is to detect forest fires in their earlier stages to prevent them\n","from spreading, prevent them from causing damage to a variety of things,\n","and most importantly, reduce or eliminate the chances of someone dying\n","from a wildfire. A fire detection system should be efficient and accurate\n","with respect to extinguishing wildfires in their earlier stages to prevent\n","the spread of them along with their consequences. Computer Vision is\n","potentially a more reliable, fast, and widespread method we need. The\n","current research in the field of preliminary fire detection has several prob-\n","lems related to unrepresentative data being used to train models and their\n","existing varied amounts of label imbalance in the classes (commonly fire\n","and non-fire) of their dataset. We propose a more representative and\n","evenly distributed data through better settings, lighting, atmospheres,\n","etc., and class distribution in the entire dataset. After thoroughly exam-\n","ining the results of this research, it can be inferred that they supported\n","the dataset’s strengths by being a viable resource when tested in the real\n","world on unfamiliar data. This is evident since as the model trains on\n","the dataset, it is able to generalize on it, hence confirming this is a viable\n","Machine Learning setting that has practical impact. Since severe droughts are occurring more frequently and lengthening the dry season in the Amazon\n","Rainforest, it is important to detect wildfires promptly and forecast possible spread for effective sup-\n","pression response. Though computer vision researchers have applied algorithms to automatically detect\n","wildfires, current models are computationally expensive and not versatile enough for the low technol-\n","ogy conditions of South American wildfire hotspots. This comprehensive deep learning study first\n","trains a Fully Convolutional Neural Network with skip connections on multispectral Landsat 8 images\n","of Ecuador and the Galapagos. The model uses Green and Short-wave Infrared (SWIR) bands as in-\n","puts to predict each image’s corresponding pixel-level binary fire mask. This model achieves a 0.962\n","validation F2 score and a 0.932 F2 score on test data from Guyana and Suriname. Afterward, image\n","segmentation is conducted on the Cirrus band using K-Means Clustering to simplify continuous pixel\n","values into three discrete classes representing differing degrees of cirrus cloud contamination. Two ad-\n","ditional Convolutional Neural Networks are trained to classify the presence of a wildfire using these\n","segmented cirrus images. The ”experimental” model trained on the segmented inputs and SWIR data\n","achieves a binary accuracy that is 2.306% higher than that of the ”benchmark model” that is trained\n","only on SWIR data. The difference in performance has a p-value of 0.00968. This proof of concept\n","reveals that feature simplification can improve the performance of wildfire detection models. Overall,\n","the software built in this study is useful for early and accurate detection of wildfires in South America.\n","The size and frequency of wildland fires in the western United States have dramatically\n","increased in recent years. On high-fire-risk days, a small fire ignition can rapidly grow and become out of\n","control. Early detection of fire ignitions from initial smoke can assist the response to such fires before they\n","become difficult to manage. Past deep learning approaches for wildfire smoke detection have suffered\n","from small or unreliable datasets that make it difficult to extrapolate performance to real-world scenarios.\n","In this work, we present the Fire Ignition Library (FIgLib), a publicly available dataset of nearly 25,000\n","labeled wildfire smoke images as seen from fixed-view cameras deployed in Southern California. We\n","also introduce SmokeyNet, a novel deep learning architecture using spatiotemporal information from\n","camera imagery for real-time wildfire smoke detection. When trained on the FIgLib dataset, SmokeyNet\n","outperforms comparable baselines and rivals human performance. We hope that the availability of the\n","FIgLib dataset and the SmokeyNet architecture will inspire further research into deep learning methods\n","for wildfire smoke detection, leading to automated notification systems that reduce the time to wildfire\n","response.\n","\"\"\""],"metadata":{"id":"5q4C_LRrcYX3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cosine Similarity"],"metadata":{"id":"Td7oa9MTC5tg"}},{"cell_type":"code","source":["def get_similarity(summary):\n","\n","  count_vect = CountVectorizer()\n","\n","  corpus = [summary, human_summary]\n","\n","  X_train_counts = count_vect.fit_transform(corpus)\n","\n","  pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names(),index=['summary','human_summary'])\n","\n","  vectorizer = TfidfVectorizer()\n","\n","  trsfm=vectorizer.fit_transform(corpus)\n","  pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=['summary','human_summary'])\n","\n","  sim = cosine_similarity(trsfm[0:1], trsfm)\n","\n","  return sim"],"metadata":{"id":"mRnxEeoQv1kE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Cosine similarity of summary generated from heapq:\",get_similarity(summary_lib))\n"],"metadata":{"id":"QgTy7ilroXfW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"12a34d0d-2a8a-4bd1-9789-9ebcae48f30c","executionInfo":{"status":"ok","timestamp":1650327039243,"user_tz":420,"elapsed":138,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity of summary generated from heapq: [[1.         0.79421573]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["print(\"Cosine similarity of summary generated from own implementation:\",get_similarity(summary))"],"metadata":{"id":"HFF4OOLIpHUn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcff720a-16d2-4942-8d0e-225dbca9cb61","executionInfo":{"status":"ok","timestamp":1650327044134,"user_tz":420,"elapsed":141,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity of summary generated from own implementation: [[1.         0.77623598]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["Rouge "],"metadata":{"id":"6SpUJQsbDLsp"}},{"cell_type":"code","source":["def get_rouge(summary):\n","  rouge = Rouge()\n","  return rouge.get_scores(summary, human_summary)"],"metadata":{"id":"8pu2gv2pdKOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Rouge scores for summary implemented through spacy and heapq\n","get_rouge(summary_lib)"],"metadata":{"id":"L0TI0tgNdPRy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c82953ed-d631-47be-83b0-54d02fdc1fa1","executionInfo":{"status":"ok","timestamp":1650327340651,"user_tz":420,"elapsed":5359,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'rouge-1': {'f': 0.1851152222224703,\n","   'p': 0.10898576512455516,\n","   'r': 0.6140350877192983},\n","  'rouge-2': {'f': 0.06593781828523634,\n","   'p': 0.037179734155268734,\n","   'r': 0.2911010558069382},\n","  'rouge-l': {'f': 0.1775594987619489,\n","   'p': 0.1045373665480427,\n","   'r': 0.5889724310776943}}]"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# Rouge scores for summary of own implementation\n","get_rouge(summary)"],"metadata":{"id":"Ws05w4W-nlKK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"84270230-7d44-4171-9db3-fdefd6982a69","executionInfo":{"status":"ok","timestamp":1650327342224,"user_tz":420,"elapsed":1576,"user":{"displayName":"Mohini Patil","userId":"16893043887180781667"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'rouge-1': {'f': 0.2867579862354163,\n","   'p': 0.22557471264367815,\n","   'r': 0.39348370927318294},\n","  'rouge-2': {'f': 0.07632093495159836,\n","   'p': 0.056480811006517015,\n","   'r': 0.11764705882352941},\n","  'rouge-l': {'f': 0.24474885381532502,\n","   'p': 0.1925287356321839,\n","   'r': 0.3358395989974937}}]"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":[""],"metadata":{"id":"vQifVXVQKIfs"},"execution_count":null,"outputs":[]}]}